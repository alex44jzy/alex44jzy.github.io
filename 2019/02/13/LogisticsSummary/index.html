<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />





  <link rel="alternate" href="/atom.xml" title="SciAes" type="application/atom+xml" />






<meta name="description" content="最近复习了一些基础知识，摘取了一些重要的做了一点推导，也为了之后可能面试会用到。应该是机器学习类的，但是也懒得分那么多类了，就先放在慢学NLP下面吧。这里选择把逻辑回归和softmax归到一起来讲，原因在于更好的比较两者的异同和联系。总结为以下两点： 逻辑回归的优化目标是极大化对数似然估计，采用梯度上升来学习及更新参数 $\theta$ 向量的值。 Softmax的优化目标是极小化交叉熵损失函数，">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistics到softmax推导整理">
<meta property="og:url" content="http://alexjiangzy.com/2019/02/13/LogisticsSummary/index.html">
<meta property="og:site_name" content="SciAes">
<meta property="og:description" content="最近复习了一些基础知识，摘取了一些重要的做了一点推导，也为了之后可能面试会用到。应该是机器学习类的，但是也懒得分那么多类了，就先放在慢学NLP下面吧。这里选择把逻辑回归和softmax归到一起来讲，原因在于更好的比较两者的异同和联系。总结为以下两点： 逻辑回归的优化目标是极大化对数似然估计，采用梯度上升来学习及更新参数 $\theta$ 向量的值。 Softmax的优化目标是极小化交叉熵损失函数，">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://i.postimg.cc/jq7GHt6r/image.png">
<meta property="og:image" content="https://i.postimg.cc/qB1CGHpj/image.png">
<meta property="og:image" content="https://i.postimg.cc/mDPHFcCC/image.png">
<meta property="og:updated_time" content="2021-10-23T11:17:42.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistics到softmax推导整理">
<meta name="twitter:description" content="最近复习了一些基础知识，摘取了一些重要的做了一点推导，也为了之后可能面试会用到。应该是机器学习类的，但是也懒得分那么多类了，就先放在慢学NLP下面吧。这里选择把逻辑回归和softmax归到一起来讲，原因在于更好的比较两者的异同和联系。总结为以下两点： 逻辑回归的优化目标是极大化对数似然估计，采用梯度上升来学习及更新参数 $\theta$ 向量的值。 Softmax的优化目标是极小化交叉熵损失函数，">
<meta name="twitter:image" content="https://i.postimg.cc/jq7GHt6r/image.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://alexjiangzy.com/2019/02/13/LogisticsSummary/"/>




<link rel="stylesheet" href="/js/prism/prism.css">


  <title>Logistics到softmax推导整理 | SciAes</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-117094882-1', 'auto');
  ga('send', 'pageview');
</script>






</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SciAes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">机器学习的自我重建之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-lab">
          <a href="/lab" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Lab
          </a>
        </li>
      
        
        <li class="menu-item menu-item-fav">
          <a href="/Fav" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Fav
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://alexjiangzy.com/2019/02/13/LogisticsSummary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alex Jiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SciAes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Logistics到softmax推导整理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-13T07:49:17+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/02/13/LogisticsSummary/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/02/13/LogisticsSummary/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>最近复习了一些基础知识，摘取了一些重要的做了一点推导，也为了之后可能面试会用到。应该是机器学习类的，但是也懒得分那么多类了，就先放在慢学NLP下面吧。这里选择把逻辑回归和softmax归到一起来讲，原因在于更好的比较两者的异同和联系。总结为以下两点：</p>
<p><strong>逻辑回归的优化目标是极大化对数似然估计，采用梯度上升来学习及更新参数 $\theta$ 向量的值。</strong></p>
<p><strong>Softmax的优化目标是极小化交叉熵损失函数，采用梯度下降和链式法则来更新各层权值。</strong></p>
<h1>Logistics Regression</h1>
<p>先从线性回归开始</p>
<p>$$h _ { w } \left( x ^ { i } \right) = w _ { 0 } + w _ { 1 } x _ { 1 } + w x _ { 2 } + \ldots + w _ { n } x _ { n }$$</p>
<p>$$h _ { w } \left( x ^ { j } \right) = w ^ { T } x _ { i } = W ^ { T } X$$</p>
<p>$$X = \left[ \begin{array} { c } { 1 } \newline { x _ { 1 } } \newline { \dots } \newline { x _ { n } } \end{array} \right] \quad W = \left[ \begin{array} { c } { w _ { 0 } } \newline { w _ { 1 } } \newline { \dots } \newline { w _ { n } } \end{array} \right]$$</p>
<p>针对线性分类器而言，他解决的是回归问题，为了能更好进行分类问题的探讨，这里就引出了 Logistics 回归。</p>
<a id="more"></a>
<h2 id="基础知识">基础知识</h2>
<p>逻辑回归是假设数据服从 Bernoulli 分布（抛硬币），因此LR属于参数模型。</p>
<p>其中是对于线性模型，加上了一个 Sigmoid 函数，这个也是神经网络的激活函数，拥有很多良好的特性。1. 拥有很好的激活特性，2. 求导很容易，这一点在 GD 上太重要了，对于 NN 的 BPTT 也起到了极为重要的作用。</p>
<p>LR 目标函数定义：$h _ { \theta } ( x ) = g \left( \theta ^ { T } x \right)$</p>
<p>其中 Sigmoid 函数 g(z) 的定义：$g ( z ) = \frac { 1 } { 1 + e ^ { - z } }$</p>
<p>Sigmoid 函数求导 $g ^ {'} (z)$ 为：</p>
<p>$$<br>
\begin{aligned} g ^ { \prime } ( z ) &amp; = \frac { d } { d z } \frac { 1 } { 1 + e ^ { - z } } \newline &amp; = \frac { 1 } { \left( 1 + e ^ { - z } \right) ^ { 2 } } \left( e ^ { - z } \right) \newline &amp; = \frac { 1 } { \left( 1 + e ^ { - z } \right) ^ { 2 } } \cdot \left( 1 - \frac { 1 } { \left( 1 + e ^ { - z } \right) } \right) \newline &amp; = g ( z ) ( 1 - g ( z ) ) \end{aligned}<br>
$$</p>
<h2 id="似然函数">似然函数</h2>
<p>假设有n个独立的训练样本 { (x1, y1), (x2, y2), … , (xn, yn)}，y={0, 1}。那每一个观察到的样本 (xi, yi) 出现的概率是：</p>
<p>$$<br>
P \left( \mathrm { y } _ { i } , \mathrm { x } _ { i } \right) = P \left( \mathrm { y } _ { i } = 1 | \mathrm { x } _ { i } \right) ^ { y _ { i } } \left( 1 - P \left( \mathrm { y } _ { i } = 1 | \mathrm { x } _ { i } \right) \right) ^ { \mathrm { 1 } - y _ { j } }<br>
$$</p>
<p>推广到所有样本下，得到整体的似然函数表达，需要将所有的样本似然函数全部相乘。这里的参数 $\theta$ 是我们要估计的参数，似然函数正比于我们的概率函数 $L ( \theta | x ) \propto P ( \operatorname { x | \theta )}$，累成后得到整体的似然函数表达：</p>
<p>$$<br>
L ( \theta ) = \prod P \left( \mathrm { y } _ { i } = 1 | \mathrm { x } _ { i } \right) ^ { y _ { i } } \left( 1 - P \left( \mathrm { y } _ { i } = 1 | \mathrm { x } _ { i } \right) \right) ^ { 1 - y _ { i } }<br>
$$</p>
<p>具体为</p>
<p>$$<br>
\begin{aligned} L ( \theta )  &amp; = p ( \vec { y } | X ; \theta ) \newline &amp; = \prod _ { i = 1 } ^ { m } p \left( y ^ { ( i ) } | x ^ { ( i ) } ; \theta \right) \newline &amp; = \prod _ { i = 1 } ^ { m } \left( h _ { \theta } \left( x ^ { ( i ) } \right) \right) ^ { y ^ { ( i ) } } \left( 1 - h _ { \theta } \left( x ^ { ( i ) } \right) \right) ^ { 1 - y ^ { ( i ) } } \end{aligned}<br>
$$</p>
<p>累乘的形式不利于进行优化分析，这里将似然函数取对数，得到对数似然函数，作为我们的最终优化目标，运用极大似然估计来求得最优的 $\theta$</p>
<p>$$<br>
\begin{aligned} \ell ( \theta ) &amp; = \log L ( \theta ) \newline &amp; = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log h \left( x ^ { ( i ) } \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right) \end{aligned}<br>
$$</p>
<h2 id="最优化求解推导">最优化求解推导</h2>
<p>利用链式法对目标函数则进行求导以求得最优参数。</p>
<p>$$<br>
\frac { \partial } { \theta _ { j } } J ( \theta ) = \frac { \partial J ( \theta ) } { \partial g \left( \theta ^ { T } x \right) } * \frac { \partial g \left( \theta ^ { T } x \right) } { \partial \theta ^ { T } x } * \frac { \partial \theta ^ { T } x } { \partial \theta _ { j } }<br>
$$</p>
<p>分三部分求导：</p>
<p>第一部分</p>
<p>$$<br>
\frac { \partial J ( \theta ) } { \partial g \left( \theta ^ { T } x \right) } = y * \frac { 1 } { g \left( \theta ^ { T } x \right) } + ( y - 1 ) * \frac { 1 } { 1 - g \left( \theta ^ { T _ { x } } x \right) }<br>
$$</p>
<p>第二部分</p>
<p>$$<br>
\frac { \partial g \left( \theta ^ { T } x \right) } { \partial \theta ^ { T } x } = g \left( \theta ^ { T } x \right) \left( 1 - g \left( \theta ^ { T } x \right) \right)<br>
$$</p>
<p>第三部分</p>
<p>$$<br>
\frac { \partial \theta ^ { T } x } { \theta _ { j } } = \frac { \partial J \left( \theta _ { 1 } x _ { 1 } + \theta _ { 2 } x _ { 2 } + \cdots \theta _ { n } x _ { n } \right) } { \partial \theta _ { j } } = x _ { j }<br>
$$</p>
<p>整理得到最终形式：</p>
<p>$$<br>
\begin{aligned} \frac { \partial } { \partial \theta _ { j } } \ell ( \theta ) &amp; = \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) \frac { \partial } { \partial \theta _ { j } } g \left( \theta ^ { T } x \right) \newline &amp; = \left( y \frac { 1 } { g \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - g \left( \theta ^ { T } x \right) } \right) g \left( \theta ^ { T } x \right) \left( 1 - g \left( \theta ^ { T } x ) \right) \right) \frac { \partial } { \partial \theta _ { j } } \theta ^ { T } x \newline &amp; = \left( y \left( 1 - g \left( \theta ^ { T } x \right) \right) - ( 1 - y ) g \left( \theta ^ { T } x \right) \right) x _ { j } \newline &amp; = \left( y - h _ { \theta } ( x ) \right) x _ { j } \end{aligned}<br>
$$</p>
<p>因此总的 $\theta$ 更新公式为：</p>
<p>$$<br>
\theta _ { j } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }<br>
$$</p>
<p><strong>速记：</strong> 对 $\theta$ 的梯度更新为输入 $x_i$ * (y真实值 - logistics 的预测计算值）</p>
<p>整个迭代过程就是最为一般的梯度下降，逻辑回归的似然函数是典型的凸函数，运用凸优化的方式可以保证求得最优解。要考LR的话就可以这么写了吧。</p>
<h2 id="扩展">扩展</h2>
<p>（1）正则化</p>
<p>上述是最基础的逻辑回归的推导形式，但是在实际中，还是要针对模型进行一些正则化，防止参数规模过大产生过拟合的情况，违背了奥卡姆剃刀。这里列出了含正则化表达式的逻辑回归目标函数：</p>
<p>$$<br>
J ( \theta ) = - \frac { 1 } { N } \sum y \log g \left( \theta ^ { T } x \right) + ( 1 - y ) \log \left( 1 - g \left( \theta ^ { T } x \right) \right) + \lambda | w | _ { p }<br>
$$</p>
<p>（2）关于优化方法，上述给出的是最为一般的梯度下降法，其他的凸优化方法还包括有：</p>
<ul>
<li>牛顿法</li>
<li>拟牛顿法</li>
<li>DFP算法</li>
<li>BGFS拟牛顿法<br>
（详见李航《统计学习方法》附录B）</li>
</ul>
<p>（3）LR 和 SVM 的异同点</p>
<p><strong>相同点：</strong></p>
<ul>
<li>LR和SVM都是分类算法。</li>
<li>如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。</li>
<li>LR和SVM都是监督学习算法，为判别模型。</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li>本质上是其loss function不同。<br>
逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。支持向量机基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面。</li>
<li>支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。</li>
<li>在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。</li>
<li>SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！</li>
<li>SVM不是概率输出，Logistic Regression是概率输出。</li>
</ul>
<p><strong>总而言之，一个基于距离！一个基于概率！</strong></p>
<hr>
<h1>Softmax Cross-Entropy 推导</h1>
<p>为什么衡量softmax多分类的损失函数要使用交叉熵来定义，我们选取比较常用的另外的几种错误率计算：</p>
<ul>
<li>
<p>Classification Error（分类错误率）：无法区分分别。</p>
</li>
<li>
<p>Mean Squared Error (均方误差)：主要原因是逻辑回归配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况（MSE损失函数），二次代价函数的不足，误差大的时候反而学的比较慢。<br>
<a href="https://postimg.cc/rzqnc6Yh" target="_blank" rel="noopener"><img src="https://i.postimg.cc/jq7GHt6r/image.png" alt="image.png"></a></p>
</li>
<li>
<p>Cross Entropy Error Function（交叉熵损失函数）：交叉熵函数是凸函数，求导时更容易找到全局最优解，函数的性质就比较好，因此学的也就更快。<br>
<a href="https://postimg.cc/Hj7xpKcy" target="_blank" rel="noopener"><img src="https://i.postimg.cc/qB1CGHpj/image.png" alt="image.png"></a></p>
</li>
</ul>
<p>明确了交叉熵的优势后，将最后一层的网络展开，最终的结果展开成 one-hot 的形式，用 softmax 得到的概率值进行交叉熵的计算，带入公式</p>
<p>$$<br>
J = - \sum _ { c = 1 } ^ { M } y _ { c } \log \left( p _ { c } \right)<br>
$$</p>
<p><a href="https://postimg.cc/BLG6rnhn" target="_blank" rel="noopener"><img src="https://i.postimg.cc/mDPHFcCC/image.png" alt="image.png"></a></p>
<p>$$<br>
\frac { \partial J } { \partial w _ { i } } = \frac { \partial J } { \partial p _ { i } } \cdot \frac { \partial p _ { i } } { \partial s c o r e _ { i } } \cdot \frac { \partial s c o r e _ { i } } { \partial w _ { i } }<br>
$$</p>
<p><strong>计算第一项：</strong></p>
<p>$$<br>
\begin{aligned} \frac { \partial J } { \partial p _ { i } } &amp; = \frac { \partial - [ y \log ( p ) + ( 1 - y ) \log ( 1 - p ) ] } { \partial p _ { i } } \newline &amp; = - \frac { \partial y _ { i } \log p _ { i } } { \partial p _ { i } } - \frac { \partial \left( 1 - y _ { i } \right) \log \left( 1 - p _ { i } \right) } { \partial p _ { i } } \newline &amp; = - \frac { y _ { i } } { p _ { i } } - \left[ \left( 1 - y _ { i } \right) \cdot \frac { 1 } { 1 - p _ { i } } \cdot ( - 1 ) \right] \newline &amp; = - \frac { y _ { i } } { p _ { i } } + \frac { 1 - y _ { i } } { 1 - p _ { i } } \newline &amp; = - \frac { y _ { i } } { \sigma \left( y _ { i } \right) } + \frac { 1 - y _ { i } } { 1 - \sigma \left( y _ { i } \right) }  \end{aligned}<br>
$$</p>
<p>计算第二项，稍微有点复杂，第一种k等于i的情况</p>
<p>$$<br>
\begin{aligned} \frac { \partial p _ { i } } { \partial s c o r e _ { i } } &amp; = \frac { \left( e ^ { y _ { i } } \right) ^ { \prime } \cdot \left( \sum _ { i } e ^ { y _ { i } } \right) - e ^ { y _ { i } } \cdot \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { \prime } } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = \frac { e ^ { y _ { i } } \cdot \sum _ { i } e ^ { y _ { i } } - \left( e ^ { y _ { i } } \right) ^ { 2 } } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = \frac { e ^ { y _ { i } } } { \sum _ { j } e ^ { y _ { i } } } - \frac { \left( e ^ { y _ { i } } \right) ^ { 2 } } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = \frac { e ^ { y _ { i } } } { \sum _ { j } e ^ { y _ { i } } } \cdot \left( 1 - \frac { e ^ { y _ { i } } } { \sum _ { j } e ^ { y _ { i } } } \right) \newline &amp; = \sigma \left( y _ { i } \right) \left( 1 - \sigma \left( y _ { i } \right) \right) \end{aligned}<br>
$$</p>
<p>第二种考虑k不等于i的情况</p>
<p>$$<br>
\begin{aligned} \frac { \partial p _ { k } } { \partial s c o r e _ { i } } &amp; = \frac { \left( e ^ { y _ { k } } \right) ^ { \prime } \cdot \left( \sum _ { i } e ^ { y _ { i } } \right) - e ^ { y _ { i } } \cdot \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { \prime } } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = \frac { 0 \cdot \sum _ { i } e ^ { y _ { i } } - \left( e ^ { y _ { i } } \right) \cdot \left( e ^ { y _ { k } } \right) } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = - \frac { e ^ { y _ { i } } \cdot e ^ { y _ { k } } } { \left( \sum _ { j } e ^ { y _ { i } } \right) ^ { 2 } } \newline &amp; = - \frac { e ^ { y _ { i } } } { \sum _ { j } e ^ { y _ { i } } } \cdot \frac { e ^ { y _ { k } } } { \sum _ { j } e ^ { y _ { i } } } \newline &amp; = - \sigma \left( y _ { k } \right) \cdot \sigma \left( y _ { k } \right) \newline &amp; = \sigma \left( y _ { k } \right) \cdot \left( 1 - \sigma \left( y _ { i } \right) \right) \end{aligned}<br>
$$</p>
<p>总和两个式子的结果，我们得到了统一的形式：<br>
$$<br>
\frac { \partial p _ { k } } { \partial s c o r e _ { i } } = \left{ \begin{array} { l l } { \sigma \left( y _ { i } \right) \left( 1 - \sigma \left( y _ { i } \right) \right) } &amp; { \text { if } k = i } \newline { \sigma \left( y _ { k } \right) \cdot \left( 1 - \sigma \left( y _ { i } \right) \right) } &amp; { \text { if } k \neq i } \end{array} \right.<br>
$$</p>
<p>即<br>
$$<br>
\frac { \partial p _ { i } } { \partial s c o r e _ { i } } = \sigma \left( y _ { i } \right) \left( 1 - \sigma \left( y _ { i } \right) \right)<br>
$$</p>
<p>第三项就是直接求导<br>
$$<br>
\frac { \partial s c o r e _ { i } } { \partial w _ { i } } = x _ { i }<br>
$$</p>
<p>综上计算统一的结果为</p>
<p>$$<br>
\begin{aligned} \frac { \partial J } { \partial w _ { i } } &amp; = \frac { \partial J } { \partial p _ { i } } \cdot \frac { \partial p _ { i } } { \partial s c o r e _ { i } } \cdot \frac { \partial s c o r e _ { i } } { \partial w _ { i } } \newline &amp; = \left[ - \frac { y _ { i } } { \sigma \left( y _ { i } \right) } + \frac { 1 - y _ { i } } { 1 - \sigma \left( y _ { i } \right) } \right] \cdot \sigma \left( y _ { i } \right) \left( 1 - \sigma \left( y _ { i } \right) \right) \cdot x _ { i } \newline &amp; = \left[ - \frac { y _ { i } } { \sigma \left( y _ { i } \right) } \cdot \sigma \left( y _ { i } \right) \cdot \left( 1 - \sigma \left( y _ { i } \right) \right) + \frac { 1 - y _ { i } } { 1 - \sigma \left( y _ { i } \right) } \cdot \sigma \left( y _ { i } \right) \cdot \left( 1 - \sigma \left( y _ { i } \right) \right) \right] \cdot x _ { i } \newline &amp; = \left[ - y _ { i } + y _ { i } \cdot \sigma \left( y _ { i } \right) + \sigma \left( y _ { i } \right) - y _ { i } \cdot \sigma \left( y _ { i } \right) \right] \cdot x _ { i } \newline &amp; = \left[ \sigma \left( y _ { i } \right) - y _ { i } \right] \cdot x _ { i } \end{aligned}<br>
$$</p>
<p>因此如果反向传播的是 $i$ 对应于真实的标签为 1，即 $y_i=1$，那么回传的梯度变化就需要减去 <strong>1</strong>, 再乘以 $x_i$。如果对应真实标签为0，那么 $y_i=0$，传递回该位置的 softmax 值乘以 $x_i$。如此一来，我们就得到了全连接层到输出层的梯度变化了，再回传其实也没有那么困难了。</p>
<h2 id="总结">总结</h2>
<p>LR 和 Softmax 是一切机器学习的基础，推导完毕后发现，其实两者还是存在很多的共性，尤其从结论来看，都是（真实-预测）* 输入，但是这个结论的得来要归功于 sigmoid 函数和 softmax 函数良好的凸性，梯度随误差增大而增大，以及易于求导三方面因素共同决定的，也佩服于先人的才学，才有了今天的发展。</p>
<p>参考：</p>
<p>https://www.jianshu.com/p/dce9f1af7bc9</p>
<p>https://zhuanlan.zhihu.com/p/35709485</p>
<p>https://zhuanlan.zhihu.com/p/25723112</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/31/2018总结展望/" rel="next" title="回首2018 -- 每天都是夏天的新加坡留学生活">
                <i class="fa fa-chevron-left"></i> 回首2018 -- 每天都是夏天的新加坡留学生活
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/07/25/分类总结/" rel="prev" title="分类总结">
                分类总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Alex Jiang</p>
              <p class="site-description motion-element" itemprop="description">身下万丈深渊，头上无尽苍穹</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">Logistics Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基础知识"><span class="nav-number">1.1.</span> <span class="nav-text">基础知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#似然函数"><span class="nav-number">1.2.</span> <span class="nav-text">似然函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最优化求解推导"><span class="nav-number">1.3.</span> <span class="nav-text">最优化求解推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩展"><span class="nav-number">1.4.</span> <span class="nav-text">扩展</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">Softmax Cross-Entropy 推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">2.1.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alex Jiang</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>



<script src="/js/prism/prism.js" async></script>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://alex44jzy.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://alexjiangzy.com/2019/02/13/LogisticsSummary/';
          this.page.identifier = '2019/02/13/LogisticsSummary/';
          this.page.title = 'Logistics到softmax推导整理';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://alex44jzy.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  

















  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
